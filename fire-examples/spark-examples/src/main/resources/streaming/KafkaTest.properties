#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# 非必须配置项：默认就是这个地址
spark.kafka.brokers.name            =       zms
# 必须配置项：kafka的topic列表，以逗号分隔
spark.kafka.topics                  =       aries_binlog_preorder
# 非必须配置项：默认为appName
spark.kafka.group.id                =       fire
spark.fire.rest.filter.enable       =       false
spark.log.level.fire_conf.com.zto.fire=     debug
spark.streaming.stopGracefullyOnShutdown=       false
spark.kafka.brokers.name2           =       zms
# 必须配置项：kafka的topic列表，以逗号分隔
spark.kafka.topics2                 =       cainiao_push_log,zto_scan_rec
# 非必须配置项：默认为appName
spark.kafka.group.id2               =       fire

spark.kafka.brokers.name3           =       zmsNew
# 必须配置项：kafka的topic列表，以逗号分隔
spark.kafka.topics3                 =       sjzn_spark_order_unique_topic
# 非必须配置项：默认为appName
spark.kafka.group.id3               =       fire

# 自定义的kafka地址
spark.kafka.brokers.name5           =       zmsNew
# 必须配置项：kafka的topic列表，以逗号分隔
spark.kafka.topics5                 =       sjzn_spark_binlog_order_topic
# 非必须配置项：默认为appName
spark.kafka.group.id5               =       fire


# ------------------- < hbase 配置 > ------------------- #
# 用于区分不同的hbase集群: batch/streaming/old
spark.hive.cluster                  =       batch
# spark的参数可以直接写在下面，都会被加载，覆盖程序中默认的配置信息
spark.speculation                   =       false
spark.streaming.concurrentJobs      =       1
spark.streaming.batch.duration      =       30