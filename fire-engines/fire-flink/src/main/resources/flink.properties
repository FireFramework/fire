#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# ----------------------------------------------- < fire 配置 > ------------------------------------------------ #
# fire框架rest接口服务最大线程数
fire.restful.max.thread                                                             =       10
# 是否启用fire的分布式数据同步功能，开启后可将JobManager端的数据分布式同步到每一个TaskManager端
fire.distribute.sync.enable                                                         =       true
# flink 引擎 arthas启动器类名
fire.analysis.arthas.launcher                                                       =       com.zto.fire.flink.plugin.FlinkArthasLauncher
# 当执行shutdown时是否调用System.exit
fire.shutdown.auto.exit                                                             =       true
# 主键配置映射管理类
fire.conf.anno.manager.class                                                        =       com.zto.fire.flink.conf.FlinkAnnoManager

# ----------------------------------------------- < flink 配置 > ----------------------------------------------- #
# flink的应用名称，为空则取类名
flink.appName                                                                       =
# kafka的groupid，为空则取类名
flink.kafka.group.id                                                                =
# bigdata表示连接大数据的kafka，zms表示连接zms的kafka集群
flink.kafka.brokers.name                                                            =
# topic列表
flink.kafka.topics                                                                  =
# 用于配置启动时的消费位点，默认取最新
flink.kafka.starting.offsets                                                        =
# 数据丢失时执行失败
flink.kafka.failOnDataLoss                                                          =       true
# 是否启用自动commit
flink.kafka.enable.auto.commit                                                      =       false
# 是否在checkpoint时记录offset值
flink.kafka.CommitOffsetsOnCheckpoints                                              =       true
# 设置从指定时间戳位置开始消费kafka
flink.kafka.StartFromTimestamp                                                      =       0
# 从topic中指定的group上次消费的位置开始消费，必须配置group.id参数
flink.kafka.StartFromGroupOffsets                                                   =       false

# 是否覆盖状态中的offset（请谨慎配置，用于kafka集群迁移等不正常状况的运维）
flink.kafka.force.overwrite.stateOffset.enable                                      =       false
# 是否在开启checkpoint的情况下强制开启周期性offset提交
flink.kafka.force.autoCommit.enable                                                 =       false
# 周期性提交offset的时间间隔（ms）
flink.kafka.force.autoCommit.Interval                                               =       30000

# flink.kafka.conf开头的配置支持所有kafka client的配置
#flink.kafka.conf.session.timeout.ms                                                =       300000
#flink.kafka.conf.request.timeout.ms                                                =       400000
# 默认的日志级别
flink.log.level                                                                     =       WARN
# flink sql配置项，以flink.sql.conf.开头将会被自动加载
#flink.sql.conf.table.exec.mini-batch.enabled                                       =       false
#flink.sql.conf.table.exec.state.ttl                                                =       0 ms
# flink sql udf注册，以flink.sql.udf.开头，以下配置的含义是：CREATE FUNCTION fireUdf AS 'com.zto.fire.examples.flink.stream.Udf'
flink.sql.udf.fireUdf                                                               =       com.zto.fire.examples.flink.stream.Udf
flink.sql.udf.fireUdf.enable                                                        =       false
# 指定在flink引擎下，可进行配置同步的子类实现
flink.fire.conf.deploy.engine                                                       =       com.zto.fire.flink.sync.SyncFlinkEngineConf
# 是否打印组装with语句后的flink sql，由于with表达式中可能含有敏感信息，默认为关闭
flink.sql.log.enable                                                                =       false
# 是否启用配置文件中with强制替换sql中已有的with表达式，如果启用，并且配置文件中有指定with配置信息，则会强制替换掉代码中sql的with列表
flink.sql_with.replaceMode.enable                                                   =       true
# 开启webui火焰图
rest.flamegraph.enabled                                                             =       true

# ----------------------------------------------- < hive 配置 > ----------------------------------------------- #
# hive 集群名称（batch离线hive/streaming 180集群hive/test本地测试hive），用于flink跨集群读取hive元数据信息
flink.hive.cluster                                                                  =
# flink所集成的hive版本号
flink.hive.version                                                                  =       1.2.0
# 默认的hive数据库
flink.default.database.name                                                         =       tmp
# 默认的hive分区字段名称
flink.default.table.partition.name                                                  =       ds
# hive的catalog名称
flink.hive.catalog.name                                                             =       hive

# ----------------------------------------------- < HBase 配置 > ----------------------------------------------- #
# 用于区分不同的hbase集群: batch/streaming/old
flink.hbase.cluster                                                                 =       batch
# 一次读写HBase的数据量
flink.hbase.batch.size                                                              =       10000


# ----------------------------------------------- < flink 参数 > ----------------------------------------------- #
# flink相关优化参数列在下面会自动被fire加载生效
flink.auto.generate.uid.enable                                                      =       true
flink.auto.type.registration.enable                                                 =       true
flink.force.avro.enable                                                             =       false
flink.force.kryo.enable                                                             =       false
flink.generic.types.enable                                                          =       true
flink.object.reuse.enable                                                           =       false
flink.auto.watermark.interval                                                       =       -1
# 默认值为：RECURSIVE，包括：RECURSIVE、NONE、TOP_LEVEL
flink.closure.cleaner.level                                                         =       recursive
flink.default.input.dependency.constraint                                           =       any
# 默认值：PIPELINED，包括：PIPELINED、PIPELINED_FORCED、BATCH、BATCH_FORCED
flink.execution.mode                                                                =       pipelined
flink.latency.tracking.interval                                                     =
flink.max.parallelism                                                               =       10240
flink.default.parallelism                                                           =
flink.task.cancellation.interval                                                    =
flink.task.cancellation.timeout.millis                                              =
flink.use.snapshot.compression                                                      =       false
flink.stream.buffer.timeout.millis                                                  =
flink.stream.number.execution.retries                                               =
flink.stream.time.characteristic                                                    =
# 是否将配置同步到taskmanager端
flink.fire.deploy_conf.enable                                                       =      false
# 默认的flink default catalog名称
flink.sql.default.catalog.name                                                      =      default_catalog