#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# ----------------------------------------------- < flink 配置 > ----------------------------------------------- #
# flink的应用名称，为空则取类名
flink.appName                                                                       =
# kafka的groupid，为空则取类名
flink.kafka.group.id                                                                =
# bigdata表示连接大数据的kafka，zms表示连接zms的kafka集群
flink.kafka.brokers.name                                                            =
# topic列表
flink.kafka.topics                                                                  =
# 用于配置启动时的消费位点，默认取最新
flink.kafka.starting.offsets                                                        =
# 数据丢失时执行失败
flink.kafka.failOnDataLoss                                                          =       true
# 是否启用自动commit
flink.kafka.enable.auto.commit                                                      =       false
# 是否在checkpoint时记录offset值
flink.kafka.CommitOffsetsOnCheckpoints                                              =       true
# 设置从指定时间戳位置开始消费kafka
flink.kafka.StartFromTimestamp                                                      =       0
# 从topic中指定的group上次消费的位置开始消费，必须配置group.id参数
flink.kafka.StartFromGroupOffsets                                                   =       false
# flink.kafka.conf开头的配置支持所有kafka client的配置
#flink.kafka.conf.session.timeout.ms                                                =       300000
#flink.kafka.conf.request.timeout.ms                                                =       400000
# 默认的日志级别
flink.log.level                                                                     =       WARN
# flink sql配置项，以flink.sql.conf.开头将会被自动加载
#flink.sql.conf.table.exec.mini-batch.enabled                                       =       false
#flink.sql.conf.table.exec.state.ttl                                                =       0 ms
# flink sql udf注册，以flink.sql.udf.开头，以下配置的含义是：CREATE FUNCTION fireUdf AS 'com.zto.fire.examples.flink.stream.Udf'
flink.sql.udf.fireUdf                                                               =       com.zto.fire.examples.flink.stream.Udf
flink.sql.udf.fireUdf.enable                                                        =       false
# 指定在flink引擎下，可进行配置同步的子类实现
flink.fire.conf.deploy.engine                                                       =       com.zto.fire.flink.conf.FlinkEngineConf
# 是否打印组装with语句后的flink sql，由于with表达式中可能含有敏感信息，默认为关闭
flink.sql.log.enable                                                                =       false
# 是否启用配置文件中with强制替换sql中已有的with表达式，如果启用，并且配置文件中有指定with配置信息，则会强制替换掉代码中sql的with列表
flink.sql_with.replaceMode.enable                                                   =       false

# ----------------------------------------------- < hive 配置 > ----------------------------------------------- #
# hive 集群名称（batch离线hive/streaming 180集群hive/test本地测试hive），用于flink跨集群读取hive元数据信息
flink.hive.cluster                                                                  =
# flink所集成的hive版本号
flink.hive.version                                                                  =       1.1.0
# 默认的hive数据库
flink.default.database.name                                                         =       tmp
# 默认的hive分区字段名称
flink.default.table.partition.name                                                  =       ds
# hive的catalog名称
flink.hive.catalog.name                                                             =       hive

# ----------------------------------------------- < HBase 配置 > ----------------------------------------------- #
# 用于区分不同的hbase集群: batch/streaming/old
flink.hbase.cluster                                                                 =       batch
# 一次读写HBase的数据量
flink.hbase.batch.size                                                              =       10000


# ----------------------------------------------- < flink 参数 > ----------------------------------------------- #
# flink相关优化参数列在下面会自动被fire加载生效
flink.auto.generate.uid.enable                                                      =       true
flink.auto.type.registration.enable                                                 =       true
flink.force.avro.enable                                                             =       false
flink.force.kryo.enable                                                             =       false
flink.generic.types.enable                                                          =       true
flink.object.reuse.enable                                                           =       false
flink.auto.watermark.interval                                                       =       -1
# 默认值为：RECURSIVE，包括：RECURSIVE、NONE、TOP_LEVEL
flink.closure.cleaner.level                                                         =       recursive
flink.default.input.dependency.constraint                                           =       any
# 默认值：PIPELINED，包括：PIPELINED、PIPELINED_FORCED、BATCH、BATCH_FORCED
flink.execution.mode                                                                =       pipelined
flink.latency.tracking.interval                                                     =
flink.max.parallelism                                                               =       -1
flink.default.parallelism                                                           =
flink.task.cancellation.interval                                                    =
flink.task.cancellation.timeout.millis                                              =
flink.use.snapshot.compression                                                      =       false
flink.stream.buffer.timeout.millis                                                  =
flink.stream.number.execution.retries                                               =
flink.stream.time.characteristic                                                    =

# checkpoint相关配置
# checkpoint频率，-1表示关闭
flink.stream.checkpoint.interval                                                    =      -1
# EXACTLY_ONCE/AT_LEAST_ONCE
flink.stream.checkpoint.mode                                                        =      EXACTLY_ONCE
# checkpoint超时时间，单位：毫秒
flink.stream.checkpoint.timeout                                                     =      600000
# 同时checkpoint操作的并发数
flink.stream.checkpoint.max.concurrent                                              =      1
# 两次checkpoint的最小停顿时间
flink.stream.checkpoint.min.pause.between                                           =      0
# 如果有更近的checkpoint时，是否将作业回退到该检查点
flink.stream.checkpoint.prefer.recovery                                             =      false
# 可容忍checkpoint失败的次数，默认不允许失败
flink.stream.checkpoint.tolerable.failure.number                                    =      0
# 当cancel job时保留checkpoint
flink.stream.checkpoint.externalized                                                =      RETAIN_ON_CANCELLATION
# 是否将配置同步到taskmanager端
flink.fire.deploy_conf.enable                                                       =      false