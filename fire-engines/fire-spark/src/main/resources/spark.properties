#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# spark的应用名称，为空则取类名
spark.appName                                                                       =
# spark local模式下使用多少core运行，默认为local[*]，自动根据当前pc的cpu核心数设置
spark.local.cores                                                                   =       *
# spark checkpoint目录地址
spark.chkpoint.dir                                                                  =       hdfs://appcluster/user/spark/ckpoint/
# 默认的spark日志级别
spark.log.level                                                                     =       WARN
spark.redaction.regex                                                               =       (?i)secret|password|map|address|namenode|connection|metastore
spark.fire.scheduler.blacklist                                                      =       jvmMonitor
# 指定在spark引擎下，可进行配置同步的子类实现
spark.fire.conf.deploy.engine                                                       =       com.zto.fire.spark.conf.SparkEngineConf

# ----------------------------------------------- < kafka 配置 > ----------------------------------------------- #
# kafka的groupid，为空则取类名
spark.kafka.group.id                                                                =
# bigdata表示连接大数据的kafka，zms表示连接zms的kafka集群
# spark.kafka.brokers.name                                                          =       bigdata
# topic列表
spark.kafka.topics                                                                  =
# 用于配置启动时的消费位点，默认取最新
spark.kafka.starting.offsets                                                        =       latest
# 数据丢失时执行失败
spark.kafka.failOnDataLoss                                                          =       true
# 是否启用自动commit
spark.kafka.enable.auto.commit                                                      =       false
# 以spark.kafka.conf开头的配置支持所有kafka client的配置
#spark.kafka.conf.session.timeout.ms                                                =       300000
#spark.kafka.conf.request.timeout.ms                                                =       400000

# ----------------------------------------------- < hive 配置 > ------------------------------------------------ #
# hive 集群名称（batch离线hive/streaming 180集群hive/test本地测试hive），用于spark跨集群读取hive元数据信息
spark.hive.cluster                                                                  =
# 以spark.hive.conf.为前缀的配置将直接生效，比如开启hive动态分区
# this.spark.sql("set hive.exec.dynamic.partition=true")
#spark.hive.conf.hive.exec.dynamic.partition                                        =       true
# spark.sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
#spark.hive.conf.hive.exec.dynamic.partition.mode                                   =       nonstrict
#spark.hive.conf.hive.exec.max.dynamic.partitions                                   =       5000

# ----------------------------------------------- < HBase 配置 > ----------------------------------------------- #
# 用于区分不同的hbase集群: batch/streaming/old/test
spark.hbase.cluster                                                                 =

# --------------------------------------------- < RocketMQ 配置 > ---------------------------------------------- #
spark.rocket.cluster.map.test                                                       =       192.168.1.169:9876;192.168.1.170:9876
# 以spark.rocket.conf开头的配置支持所有rocket client的配置
#spark.rocket.conf.pull.max.speed.per.partition                                     =       5000

# ----------------------------------------------- < impala 配置 > ---------------------------------------------- #
spark.impala.connection.url                                                         =      jdbc:hive2://192.168.25.37:21050/;auth=noSasl
spark.impala.jdbc.driver.class.name                                                 =      org.apache.hive.jdbc.HiveDriver

# ----------------------------------------------- < spark 参数 > ----------------------------------------------- #
# Spark相关优化参数列在下面会自动被fire加载生效