#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# ----------------------------------------------- < fire \u914D\u7F6E > ------------------------------------------------ #
# flink \u5F15\u64CE arthas\u542F\u52A8\u5668\u7C7B\u540D
fire.analysis.arthas.launcher                                                       =       com.zto.fire.spark.plugin.SparkArthasLauncher
# \u4E3B\u952E\u914D\u7F6E\u6620\u5C04\u7BA1\u7406\u7C7B
fire.conf.anno.manager.class                                                        =       com.zto.fire.spark.conf.SparkAnnoManager

# ----------------------------------------------- < spark \u914D\u7F6E > ----------------------------------------------- #
# spark\u7684\u5E94\u7528\u540D\u79F0\uFF0C\u4E3A\u7A7A\u5219\u53D6\u7C7B\u540D
spark.appName                                                                       =
# spark local\u6A21\u5F0F\u4E0B\u4F7F\u7528\u591A\u5C11core\u8FD0\u884C\uFF0C\u9ED8\u8BA4\u4E3Alocal[*]\uFF0C\u81EA\u52A8\u6839\u636E\u5F53\u524Dpc\u7684cpu\u6838\u5FC3\u6570\u8BBE\u7F6E
spark.local.cores                                                                   =       *
# spark checkpoint\u76EE\u5F55\u5730\u5740
spark.chkpoint.dir                                                                  =       hdfs://appcluster/user/spark/ckpoint/
# \u9ED8\u8BA4\u7684spark\u65E5\u5FD7\u7EA7\u522B
spark.log.level                                                                     =       WARN
spark.redaction.regex                                                               =       (?i)secret|password|map|address|namenode|connection|metastore
spark.fire.scheduler.blacklist                                                      =       jvmMonitor
# \u6307\u5B9A\u5728spark\u5F15\u64CE\u4E0B\uFF0C\u53EF\u8FDB\u884C\u914D\u7F6E\u540C\u6B65\u7684\u5B50\u7C7B\u5B9E\u73B0
spark.fire.conf.deploy.engine                                                       =       com.zto.fire.spark.sync.SyncSparkEngine
# \u662F\u5426\u542F\u7528sql\u6269\u5C55\u7528\u4E8E\u8840\u7F18\u91C7\u96C6
spark.fire.sql.extensions.enable                                                    =       true
# stage \u5931\u8D25\u7684\u6700\u5927\u6B21\u6570\uFF0C\u5C0F\u4E8E\u7B49\u4E8E\u96F6\u8868\u793A\u4E0D\u5F00\u542F
spark.fire.stage.maxFailures                                                        =       -1

# ----------------------------------------------- < kafka \u914D\u7F6E > ----------------------------------------------- #
# kafka\u7684groupid\uFF0C\u4E3A\u7A7A\u5219\u53D6\u7C7B\u540D
spark.kafka.group.id                                                                =
# bigdata\u8868\u793A\u8FDE\u63A5\u5927\u6570\u636E\u7684kafka\uFF0Czms\u8868\u793A\u8FDE\u63A5zms\u7684kafka\u96C6\u7FA4
# spark.kafka.brokers.name                                                          =       bigdata
# topic\u5217\u8868
spark.kafka.topics                                                                  =
# \u7528\u4E8E\u914D\u7F6E\u542F\u52A8\u65F6\u7684\u6D88\u8D39\u4F4D\u70B9\uFF0C\u9ED8\u8BA4\u53D6\u6700\u65B0
spark.kafka.starting.offsets                                                        =       latest
# \u6570\u636E\u4E22\u5931\u65F6\u6267\u884C\u5931\u8D25
spark.kafka.failOnDataLoss                                                          =       true
# \u662F\u5426\u542F\u7528\u81EA\u52A8commit
spark.kafka.enable.auto.commit                                                      =       false
# \u4EE5spark.kafka.conf\u5F00\u5934\u7684\u914D\u7F6E\u652F\u6301\u6240\u6709kafka client\u7684\u914D\u7F6E
#spark.kafka.conf.session.timeout.ms                                                =       300000
#spark.kafka.conf.request.timeout.ms                                                =       400000

# ----------------------------------------------- < hive \u914D\u7F6E > ------------------------------------------------ #
# hive \u96C6\u7FA4\u540D\u79F0\uFF08batch\u79BB\u7EBFhive/streaming 180\u96C6\u7FA4hive/test\u672C\u5730\u6D4B\u8BD5hive\uFF09\uFF0C\u7528\u4E8Espark\u8DE8\u96C6\u7FA4\u8BFB\u53D6hive\u5143\u6570\u636E\u4FE1\u606F
spark.hive.cluster                                                                  =
# \u4EE5spark.hive.conf.\u4E3A\u524D\u7F00\u7684\u914D\u7F6E\u5C06\u76F4\u63A5\u751F\u6548\uFF0C\u6BD4\u5982\u5F00\u542Fhive\u52A8\u6001\u5206\u533A
# this.spark.sql("set hive.exec.dynamic.partition=true")
#spark.hive.conf.hive.exec.dynamic.partition                                        =       true
# spark.sqlContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
#spark.hive.conf.hive.exec.dynamic.partition.mode                                   =       nonstrict
#spark.hive.conf.hive.exec.max.dynamic.partitions                                   =       5000

# ----------------------------------------------- < HBase \u914D\u7F6E > ----------------------------------------------- #
# \u7528\u4E8E\u533A\u5206\u4E0D\u540C\u7684hbase\u96C6\u7FA4: batch/streaming/old/test
spark.hbase.cluster                                                                 =

# --------------------------------------------- < RocketMQ \u914D\u7F6E > ---------------------------------------------- #
spark.rocket.cluster.map.test                                                       =       rocket01:9876;rocket02:9876
# \u4EE5spark.rocket.conf\u5F00\u5934\u7684\u914D\u7F6E\u652F\u6301\u6240\u6709rocket client\u7684\u914D\u7F6E
#spark.rocket.conf.pull.max.speed.per.partition                                     =       5000

# ----------------------------------------------- < impala \u914D\u7F6E > ---------------------------------------------- #
spark.impala.connection.url                                                         =      jdbc:hive2://hive-server:21050/;auth=noSasl
spark.impala.jdbc.driver.class.name                                                 =      org.apache.hive.jdbc.HiveDriver

# ----------------------------------------------- < spark \u53C2\u6570 > ----------------------------------------------- #
# Spark\u76F8\u5173\u4F18\u5316\u53C2\u6570\u5217\u5728\u4E0B\u9762\u4F1A\u81EA\u52A8\u88ABfire\u52A0\u8F7D\u751F\u6548